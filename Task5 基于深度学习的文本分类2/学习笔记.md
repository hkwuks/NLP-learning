### TextCNN



TextCNN利用CNN（卷积神经网络）进行文本特征抽取，不同大小的卷积核分别抽取n-gram特征，卷积计算出的特征图经过MaxPooling保留最大的特征值，然后将拼接成一个向量作为文本的表示。



这里我们基于TextCNN原始论文的设定，分别采用了100个大小为2,3,4的卷积核，最后得到的文本向量大小为100*3=300维。



#### 维度分析：



**input** 层（m句sentence，句子长度为n） ： m=batch_size
→ 进入 **embedding** 层 ， 变为 m×n×300 。Conv2d 只能做四维卷积，要把第1维unsqueeze，， 变为 m×1×n×300，这里也可以尝试Conv1d

→ 进入 **conv** 层 被卷积，一个大小为(2,300) 的二维卷积核 之后变为 m×1×(n-1)×1（无padding），若是大小为3变为  m×1×(n-2)×1（无padding）
→ 进入 **pooling** 层 ，每一个向量 变为 m×1×1，取了300个卷积核（out_channels），所以维度为 m×1×300
→ 进入 **classification** 层，输出各标签的概率 m×num_class。



**细节**：在batch处理的时候，每个batch一起计算，为了保证kernel卷积出来的长度相等，所以每个batch padding成一个长度几乎是必需的。

### TextRNN



有了上面的框架之后我们只需要改掉 我们的分类器TextCNN变为 TextRNN 即可。



让我们来做一下维度分析：



#### 维度分析：



这里我们首先采用已经 padding 和截断过的长度，所以每个batch都是一样的。（因为不用写代码……）



**input** 层（m句sentence，句子长度为n） ：m=batch_size
→ 进入 **embedding** 层 ， 变为 `m×n×300` 。
→ 进入 **LSTM**  变为 `m×n×hidden_size`, 这里的`hidden_size`是`LSTM`输出的维度
→ 我们只取最后一个长度作为输出，维度变为 `m×hidden_size`
→ 进入 **classification** 层(全连接层)，输出各标签的概率 `m× num_class`。